{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6b8da7f",
   "metadata": {},
   "source": [
    "# Tutorial 4: Building an End-To-End Multilingual Chat Application\n",
    "\n",
    "Welcome the last tutorial of our course our class on \"Practical Machine Translation for Low Resource Languages\". Today, we will be learning how to make a Chat-Bot that answers Grade School level math word problems using the [OpenAI API](https://openai.com/blog/openai-api) and pair it with translation models that we have trained/evaluated in the previous classes to extend the capabilities of our bot on languages other than English.\n",
    "\n",
    "Before geting started, we recommend signing up for a free-trial of the [OpenAI API](https://openai.com/blog/openai-api), which should give you free credits workth 18$ for three months. This should be plenty for the tutorial today and for your final projects. After signing up for the API, get the api key and place it in the `key.txt` file located in the same directory. Once that's setup you can proceed with the tutorial.\n",
    "\n",
    "\n",
    "\n",
    "![assets/matt.png](assets/MattDemo.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a580991",
   "metadata": {},
   "source": [
    "As always we will start by installing the libraries we will be making use of today. Particularly we will install openai API to make call to GPT-3.5 models and [langchain](https://github.com/hwchase17/langchain) to augment the GPT-3.5 with capabilities to address some of its limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907d75e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: openai in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (0.27.2)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.20 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from requests>=2.20->openai) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<5.0,>=4.0.0a3 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied, skipping upgrade: aiosignal>=1.1.2 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: frozenlist>=1.1.1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Collecting langchain==0.0.96\n",
      "  Using cached langchain-0.0.96-py3-none-any.whl (315 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (0.5.7)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (6.0)\n",
      "Requirement already satisfied: deeplake<4.0.0,>=3.2.9 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (3.2.11)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (1.24.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (1.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (2.28.2)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (1.4.46)\n",
      "Requirement already satisfied: aleph-alpha-client<3.0.0,>=2.15.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (2.16.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from langchain==0.0.96) (8.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.96) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.96) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.96) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.96) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.96) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.96) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.96) (1.3.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.96) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.96) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.96) (0.8.0)\n",
      "Requirement already satisfied: click in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (8.1.3)\n",
      "Requirement already satisfied: humbug>=0.2.6 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.2.8)\n",
      "Requirement already satisfied: boto3 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (1.26.81)\n",
      "Requirement already satisfied: numcodecs in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.11.0)\n",
      "Requirement already satisfied: pillow in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (9.4.0)\n",
      "Requirement already satisfied: tqdm in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (4.64.1)\n",
      "Requirement already satisfied: pyjwt in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (2.6.0)\n",
      "Requirement already satisfied: hub>=2.8.7 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (3.0.1)\n",
      "Requirement already satisfied: pathos in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from pydantic<2,>=1->langchain==0.0.96) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from requests<3,>=2->langchain==0.0.96) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from requests<3,>=2->langchain==0.0.96) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from requests<3,>=2->langchain==0.0.96) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))) in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from SQLAlchemy<2,>=1->langchain==0.0.96) (2.0.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain==0.0.96) (0.13.2)\n",
      "Requirement already satisfied: aiodns>=3.0.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain==0.0.96) (3.0.0)\n",
      "Requirement already satisfied: aiohttp-retry>=2.8.3 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain==0.0.96) (2.8.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.96) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.96) (1.0.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from boto3->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from boto3->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (1.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: botocore<1.30.0,>=1.29.81 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from boto3->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (1.29.81)\n",
      "Requirement already satisfied: entrypoints in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from numcodecs->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.4)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from pathos->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from pathos->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from pathos->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (0.3.6)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from pathos->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (1.7.6.6)\n",
      "Requirement already satisfied: pycares>=4.0.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain==0.0.96) (4.3.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.81->boto3->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.5.0 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain==0.0.96) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.81->boto3->deeplake<4.0.0,>=3.2.9->langchain==0.0.96) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain==0.0.96) (2.21)\n",
      "Installing collected packages: langchain\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.1\n",
      "    Uninstalling langchain-0.0.1:\n",
      "      Successfully uninstalled langchain-0.0.1\n",
      "Successfully installed langchain-0.0.96\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai\n",
    "!pip install langchain==0.0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60700767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac42db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the key\n",
    "with open(\"key.txt\") as f:\n",
    "    openai.api_key = f.read().split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f0f4a",
   "metadata": {},
   "source": [
    "We will start by creating a math word problems question answering system in English using GPT-3.5. GPT is a pre-trained Large Language Model (LLM) with 175 Billion parameters, which is trained on a huge amount of unlabelled data using the language modelling objective (i.e. given k tokens, generate (k+1)th token). While this forms the basis of all GPT family of models, GPT-3.5 is based on [InstructGPT](https://arxiv.org/abs/2203.02155), which further adds an Instruction Tuning step that learns from human feedback to follow provided instructions.\n",
    "\n",
    "![assets/instructgpt.png](assets/instructgpt.png)\n",
    "*From the [Ouyang et al. 2022](https://arxiv.org/abs/2203.02155)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b661284",
   "metadata": {},
   "source": [
    "As a consequence of the pre-training with language modeling objective and instruction tuning, we can use GPT-3.5 to complete a given piece of text and provide specific instructions about how to go about completing the text. We achieve this by defining a text prompt which is to be given as the input to the LLM which then generates a completion of the provided text. Below we define a prompt template, containing instructions for our bot and provide a few examples of the type of problems it is supposed to solve. Finally, we define a placeholder {input} which is to be replace by different questions that user may ask followed by the text `Matt:` to instruct the model to generate response as the assistant `Matt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef8f99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
    "\n",
    "If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
    "\n",
    "This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
    "\n",
    "A few examples of a potential conversation can be:\n",
    "\n",
    "Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "Matt: The answer is 11.\n",
    "\n",
    "Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
    "\n",
    "Matt: The answer is 29.\n",
    "\n",
    "Human: {input}\n",
    "Matt:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb337a",
   "metadata": {},
   "source": [
    "We will now see how we can hit the OpenAI API with a prompt to obtain the response from GPT-3.5. Below, we have implemented a function `answer_question` that does exactly that by performing two main steps:\n",
    "\n",
    "1. Inserts the question asked by the user into the prompt template to construct the prompt to be fed to the LLM\n",
    "2. Calls `openai.Completion.create` with the prompt to obtain the LLM's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "732ab9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    \"\"\"Given a question asked by the user, hits the OpenAI API with a prompt to obtain the response from GPT-3.5.\n",
    "    \n",
    "    Inputs:\n",
    "        - question (str): Question asked by the user\n",
    "        \n",
    "    Returns:\n",
    "        (str): LLMs response\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Insert the question asked by the user into the prompt template to construct the prompt to be fed to the LLM\n",
    "    prompt = template.replace(\"{input}\", question)\n",
    "    \n",
    "    # Step 2: Call `openai.Completion.create` with the prompt to obtain the LLM's response.\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", #text-davinci-003 is the GPT-3.5 model\n",
    "        prompt=prompt,\n",
    "        temperature=0,# Useful for decoding by LLM, a temperature of 0 means fully deterministic greedy decoding\n",
    "        max_tokens=100 # Maximum number of tokens that the LLM should generate\n",
    "    )\n",
    "    # The API returns other meta data as well, we are just concerened with the response text that can be obtained by doing:\n",
    "    return response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5bd9ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer is 39.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b52b7c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer is 9.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8180f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I'm a Math Assistant and it's out of my capabilities to answer that question.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"What is the best laptop in the market currently?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb9536c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, that is out of my capabilities. I'm only a Math Assistant and I can only answer simple grade school math word problems involving simple operators like addition, subtraction, division and multiplication. I advise you to take help from your teachers or parents.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"For which real numbers x do the vectors: (x, 1, 1, 1), (1, x, 1, 1), (1, 1, x, 1), (1, 1, 1, x) not form a basis of R4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f144c63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer is $12.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b52f59",
   "metadata": {},
   "source": [
    "Look closely at the response above an you will realize that the LLM produces an incorrect response to the problem. We shall now see how to make our LLM more capable for solving such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df2929",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting\n",
    "\n",
    "One way to improve the reasoning capabilities of LLMs is [Chain-of-Thought (CoT) Prompting](https://arxiv.org/abs/2201.11903). In CoT we prompt the model to provide intermediate reasoning steps before predicting the final answer. This simple stragegy has been shown to improve the performance of LLMs (mainly the ones with > 100 billion parameters) significantly!\n",
    "\n",
    "\n",
    "![assets/cot.png](assets/cot.png)\n",
    "*From the [Wei et al. 2022](https://arxiv.org/abs/2201.11903)*\n",
    "\n",
    "In the figure above, as you can see GPT with 175 billion parameters becomes significantly better at solving GSM8K task that involves solving grade school math word problems, which is exactly what we are try to do with our bot. Below we show how we can use CoT for our Math Assistant. We simply need to modify our prompt and provide intermediate steps of reasoning for the two examples that we provided before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d296a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
    "\n",
    "If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
    "\n",
    "This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
    "\n",
    "A few examples of a potential conversation can be:\n",
    "\n",
    "Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "\n",
    "Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
    "Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29..\n",
    "\n",
    "Human: {input}\n",
    "Matt:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e00375f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olivia started with $23. She spent $3 for each bagel, so she spent 5 * 3 = 15. 23 - 15 = 8. The answer is 8.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0917ea",
   "metadata": {},
   "source": [
    "As can be seen the model is now able to solve the problem perfectly. Another consequence of CoT here is that response obtained is much more detailed and can be useful for both a better user experience as well as a better understanding if the model takes the correct steps to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1048dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olivia had $23593456. She spent $3 * 56 = $168 on bagels. She has $23593456 - $168 = $23593280 left. The answer is $23593280.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"Olivia has $23593456. She bought fifty six bagels for $3 each. How much money does she have left?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b21fa5",
   "metadata": {},
   "source": [
    "And just when we thought we were done, we see another limitation of LLMs here. Particularly, here as you can see the LLM takes the correct reasoning steps and even predicts the correct equation `$23593456 - $168`, but gets the incorrect answer upon calculation. That is because LLMs are limited when it comes to performing mathematical operations with higher digit numbers.\n",
    "\n",
    "![assets/gpt3mat.png](assets/gpt3mat.png)\n",
    "*From the [Brown et al. 2020](https://arxiv.org/abs/2005.14165)*\n",
    "\n",
    "As can be seen in the plot above, while GPT with 175 billion parameters can solve up to 3-digit subtraction problem, the performance goes significantly down from 4 digits and above. We will next see how we can resolve limitations like these for such models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4820f0f",
   "metadata": {},
   "source": [
    "## Chaining LLMs with other tools to improve their limitations\n",
    "\n",
    "One observation that we can make from above is that the LLM did predict the reasoning steps and the equation for solving the problem correctly, which is the more complex part of solving these problems using computers. The actual execution of the equation that it failed at performing is something for which we already have tools like to do perfectly. Hence, we can prompt the LLM to just do what it is good at i.e. predicting the equation in this particular case and then use an external tool like a calculator or a python shell to execute the predicted equation. Below, we particularly prompt the LLM to predict the python code for solving the problem, which we can send to a python shell to execute and obtain the answer. This particular task of converting text description into an executable form is called [Semantic Parsing](https://en.wikipedia.org/wiki/Semantic_parsing), and is a well studied problem in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e9abe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_template = \"\"\"Your name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
    "\n",
    "If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
    "\n",
    "This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
    "\n",
    "A few examples of a potential conversation can be:\n",
    "\n",
    "Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. PYTHON -> print(5 + 6)\n",
    "\n",
    "\n",
    "Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
    "Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so PYTHON -> print(4*5 + 9)\n",
    "\n",
    "Human: {input}\n",
    "Matt:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a279cf",
   "metadata": {},
   "source": [
    "As can be seen now we have modified the prompt to answer in terms of python programs instead of the final answer of computation. Let's see if the model is able to generate the right program for execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1132e4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olivia started with $23593456. She bought 56 bagels for $3 each. That means she spent $3 * 56 = $168. PYTHON -> print(23593456 - 168).'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"Olivia has $23593456. She bought fifty six bagels for $3 each. How much money does she have left?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd094faf",
   "metadata": {},
   "source": [
    "As expected it does. We will now make use of the [langchain](https://github.com/hwchase17/langchain) library which provides many tools that can be combined with the LLMs to make them even more powerful. Here, we will be using the `python_repl` tool which acts as a python shell and can be used to execute any valid python command. Langchain has a bunch of other useful tools as well, and you can learn more about them [here](https://langchain.readthedocs.io/en/latest/modules/agents/tools.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "08180e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a756d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"python_repl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d3ebf132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Python REPL', description='A Python shell. Use this to execute python commands. Input should be a valid python command. If you expect output it should be printed out.', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fc1c1384a90>, func=<bound method PythonREPL.run of <langchain.python.PythonREPL object at 0x7fc1a91eaaf0>>, coroutine=None)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "36320c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_shell = tools[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b91305b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23593288\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_shell(\"print(23593456 - 168)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c8318",
   "metadata": {},
   "source": [
    "We can now re-define our `aswer_question` function as `answer_question_with_python` that generates the response in terms of a python program, executes the program, and appends the output of Python shell to the final response to display to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "17de5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_with_python(question):\n",
    "    \n",
    "    prompt = sp_template.replace(\"{input}\", question)\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    predicted_text = response[\"choices\"][0][\"text\"].strip()\n",
    "    \n",
    "    if \"PYTHON ->\" in predicted_text:\n",
    "        python_code = predicted_text.split(\"PYTHON ->\")[-1].strip()\n",
    "        answer = python_shell(python_code).strip()\n",
    "        return predicted_text.replace(f\"PYTHON -> {python_code}\", f\"The answer is {answer}\")\n",
    "    else:\n",
    "        return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5922098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(23593456 - 168)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olivia started with $23593456. She bought 56 bagels for $3 each. That means she spent 56 * 3 = 168 dollars. The answer is 23593288'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question_with_python(\"Olivia has $23593456. She bought fifty six bagels for $3 each. How much money does she have left?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c6357",
   "metadata": {},
   "source": [
    "As can be sen the model now generates a correct response of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a8899806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leah had 32 chocolates and her sister had 42. If they ate 35, they have 32 + 42 - 35 = 39 chocolates left in total. The answer is 39'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question_with_python(\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5abd0977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I'm only a Math Assistant and this question is out of my capabilities. I advise you to take help from your teachers or parents.\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"For which real numbers x do the vectors: (x, 1, 1, 1), (1, x, 1, 1), (1, 1, x, 1), (1, 1, 1, x) not form a basis of R4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2f22a",
   "metadata": {},
   "source": [
    "## Having conversations with LLM\n",
    "\n",
    "Uptil now what we have is essentially a question answering system, from which we can ask multiple questions but cannot really have conversations, as it doesn't really remember the previous questions asked and answers it generated. However, it is very easy to implement such a behavior. We can simply store the conversation history till a particular instant and provide it as part of the prompt to the model so it can be aware of the previous turns of interaction. Doing this we can now ask cross questions about a particular problem, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "644d83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathAssistant:\n",
    "    \n",
    "    \"\"\"We will define a MathAssistant class which will store the history as one of its attributes\n",
    "       so that it can remember the earlier conversations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # The template now has a placeholder for `{history}`\n",
    "        self.template = \"\"\"Your name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
    "        If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
    "        \n",
    "        This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
    "        \n",
    "        A few examples of a potential conversation can be:\n",
    "        \n",
    "        Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "        \n",
    "        Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "        \n",
    "        Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
    "        \n",
    "        Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29..\n",
    "        \n",
    "        Human: What if there were 25 computers in the server room in the beginning.\n",
    "        \n",
    "        Matt: If there were 25 computers in the beginning, the total will be 20 + 25 = 45. The answer is 45.\n",
    "        \n",
    "        {history}\n",
    "        \n",
    "        Human: {input}\n",
    "        Matt:\"\"\"\n",
    "        \n",
    "        \n",
    "        # Defining the history\n",
    "        self.history = \"\"\n",
    "        \n",
    "    def reset(self):\n",
    "        self.history = \"\"\n",
    "        \n",
    "    def __call__(self, question):\n",
    "        \n",
    "        prompt = self.template.replace(\"{history}\", self.history)\n",
    "        prompt = prompt.replace(\"{input}\", question)\n",
    "        print(prompt)\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            temperature=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        response =  response[\"choices\"][0][\"text\"].strip()\n",
    "        \n",
    "        # Update history\n",
    "        self.history += f\"\\n\\nHuman: {question}\\n\\nMatt: {response}\"\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c4c0767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matt = MathAssistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "59cc8d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
      "        If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
      "        \n",
      "        This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
      "        \n",
      "        A few examples of a potential conversation can be:\n",
      "        \n",
      "        Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "        \n",
      "        Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "        \n",
      "        Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
      "        \n",
      "        Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29..\n",
      "        \n",
      "        Human: What if there were 25 computers in the server room in the beginning.\n",
      "        \n",
      "        Matt: If there were 25 computers in the beginning, the total will be 20 + 25 = 45. The answer is 45.\n",
      "        \n",
      "        \n",
      "        \n",
      "        Human: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "        Matt:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olivia started with $23. She spent $3 for each bagel, so she spent $3 * 5 = $15. She has $23 - $15 = $8 left. The answer is $8.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt(\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "705b6c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
      "        If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
      "        \n",
      "        This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
      "        \n",
      "        A few examples of a potential conversation can be:\n",
      "        \n",
      "        Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "        \n",
      "        Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "        \n",
      "        Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
      "        \n",
      "        Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29..\n",
      "        \n",
      "        Human: What if there were 25 computers in the server room in the beginning.\n",
      "        \n",
      "        Matt: If there were 25 computers in the beginning, the total will be 20 + 25 = 45. The answer is 45.\n",
      "        \n",
      "        \n",
      "\n",
      "Human: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "\n",
      "Matt: Olivia started with $23. She spent $3 for each bagel, so she spent $3 * 5 = $15. She has $23 - $15 = $8 left. The answer is $8.\n",
      "        \n",
      "        Human: What if she had bought 2 bagels instead of 5?\n",
      "        Matt:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If she had bought 2 bagels instead of 5, she would have spent $3 * 2 = $6. She would have $23 - $6 = $17 left. The answer is $17.'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt(\"What if she had bought 2 bagels instead of 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a1022211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
      "        If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
      "        \n",
      "        This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
      "        \n",
      "        A few examples of a potential conversation can be:\n",
      "        \n",
      "        Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "        \n",
      "        Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "        \n",
      "        Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
      "        \n",
      "        Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29..\n",
      "        \n",
      "        Human: What if there were 25 computers in the server room in the beginning.\n",
      "        \n",
      "        Matt: If there were 25 computers in the beginning, the total will be 20 + 25 = 45. The answer is 45.\n",
      "        \n",
      "        \n",
      "\n",
      "Human: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "\n",
      "Matt: Olivia started with $23. She spent $3 for each bagel, so she spent $3 * 5 = $15. She has $23 - $15 = $8 left. The answer is $8.\n",
      "\n",
      "Human: What if she had bought 2 bagels instead of 5?\n",
      "\n",
      "Matt: If she had bought 2 bagels instead of 5, she would have spent $3 * 2 = $6. She would have $23 - $6 = $17 left. The answer is $17.\n",
      "        \n",
      "        Human: And what if she had 20$ to begin with?\n",
      "        Matt:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If she had 20$ to begin with, she would have spent $3 * 5 = $15. She would have 20 - 15 = $5 left. The answer is $5.'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt(\"And what if she had 20$ to begin with?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704c469",
   "metadata": {},
   "source": [
    "Equivalently, we can also use langchain for defining such behavior. It will automatically take care of storing previous history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ef891f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "class MathAssistantWithLangChain(MathAssistant):\n",
    "    \n",
    "    def __init__(self, max_memory = 2, verbose = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_memory = max_memory\n",
    "        self.verbose = verbose\n",
    "        self.template = PromptTemplate(\n",
    "            input_variables = [\"history\", \"input\"],\n",
    "            template=self.template\n",
    "        )\n",
    "    \n",
    "        self.llm_chain = LLMChain(\n",
    "            llm=OpenAI(\n",
    "                model_name=\"text-davinci-003\",\n",
    "                temperature=0,\n",
    "                max_tokens=100,\n",
    "                openai_api_key = openai.api_key\n",
    "            ),\n",
    "            prompt=self.template,\n",
    "            verbose=self.verbose,\n",
    "            memory=ConversationBufferWindowMemory(k=self.max_memory)\n",
    "        )\n",
    "        \n",
    "    def reset(self):\n",
    "        self.llm_chain = LLMChain(\n",
    "            llm=OpenAI(\n",
    "                model_name=\"text-davinci-003\",\n",
    "                temperature=0,\n",
    "                max_tokens=100,\n",
    "                openai_api_key = openai.api_key\n",
    "            ),\n",
    "            prompt=self.template,\n",
    "            verbose=self.verbose,\n",
    "            memory=ConversationBufferWindowMemory(k=self.max_memory)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def __call__(self, question):\n",
    "        return self.llm_chain.predict(input=question)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5f71e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matt = MathAssistantWithLangChain(2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ab3ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
      "        If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
      "        \n",
      "        This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
      "        \n",
      "        A few examples of a potential conversation can be:\n",
      "        \n",
      "        Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "        \n",
      "        Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "        \n",
      "        Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
      "        \n",
      "        Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29..\n",
      "        \n",
      "        Human: What if there were 25 computers in the server room in the beginning.\n",
      "        \n",
      "        Matt: If there were 25 computers in the beginning, the total will be 20 + 25 = 45. The answer is 45.\n",
      "        \n",
      "        \n",
      "        \n",
      "        Human: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "        Matt:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Olivia started with $23. She spent $3 for each bagel, so she spent $3 * 5 = $15. She has $23 - $15 = $8 left. The answer is $8.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt(\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9073254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour name is Matt, a virtual assistant for answering simple grade school math word problems involving simple operators like addition , subtraction, division and multipication.\n",
      "        If the math problem asked is more complex than what specified above, like if it involves concepts from probability or calculus, try answering that but warn the user that it might be out of your capabilities and you advise the user to take help from their teachers or parents.\n",
      "        \n",
      "        This is the only thing that you are capable of. If the user asks about some topic other than math, just say that you are only Math Assistant and it is out of your capability to answer the question.\n",
      "        \n",
      "        A few examples of a potential conversation can be:\n",
      "        \n",
      "        Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "        \n",
      "        Matt: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "        \n",
      "        Human: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\n",
      "        \n",
      "        Matt: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29..\n",
      "        \n",
      "        Human: What if there were 25 computers in the server room in the beginning.\n",
      "        \n",
      "        Matt: If there were 25 computers in the beginning, the total will be 20 + 25 = 45. The answer is 45.\n",
      "        \n",
      "        Human: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "AI:  Olivia started with $23. She spent $3 for each bagel, so she spent $3 * 5 = $15. She has $23 - $15 = $8 left. The answer is $8.\n",
      "        \n",
      "        Human: What if she had bought 2 bagels instead of 5?\n",
      "        Matt:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' If she had bought 2 bagels instead of 5, she would have spent $3 * 2 = $6. She would have $23 - $6 = $17 left. The answer is $17.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt(\"What if she had bought 2 bagels instead of 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2790053",
   "metadata": {},
   "source": [
    "## Making the Assistant work for Languages other than English\n",
    "\n",
    "For the final part of the tutorial we will be making are Math Assistant to work for languages other than English. Particularly, we will be demonstrating the use-case for Hindi. Since, GPT models are predominantly trained with English data, their capabilities for other languages, especially low-resource languages are very limited. One way to improve their capabilities in other languages is to augment these models with Machine Translation (MT) systems. It mainly involves three steps:\n",
    "\n",
    "1. Use an MT system to first translate the user query to English\n",
    "2. Feed the translated query to LLM to obtain response in English\n",
    "3. Translate the English response to the user's language\n",
    "\n",
    "We will be using  transformers library to load translation pipelines as demonstrated in the Tutorial 1 and in particular will be using the [M2M model by Meta](https://about.fb.com/news/2020/10/first-multilingual-machine-translation-model/) that is able to translate to and from 100 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8770e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89e302bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualMathAssistant(MathAssistantWithLangChain):\n",
    "    \n",
    "    def __init__(self, lang, max_memory = 2, verbose = False):\n",
    "        super().__init__(max_memory, verbose)\n",
    "        \n",
    "        self.lang = lang\n",
    "        self.translation_pipeline = pipeline('text2text-generation', model=\"facebook/m2m100_418M\")\n",
    "        \n",
    "    def translate_lang_to_en(self, text):\n",
    "        \n",
    "        # For M2M, we need to specify the langauge to translate to as a BOS token to the decoder i.e. in this case \"en\"\n",
    "        return self.translation_pipeline(text, forced_bos_token_id = pipe.tokenizer.get_lang_id('en'))[0][\"generated_text\"]\n",
    "    \n",
    "    def translate_en_to_lang(self, text):\n",
    "        \n",
    "        # For M2M, we need to specify the langauge to translate to as a BOS token to the decoder i.e. in this case `self.lang`\n",
    "        return self.translation_pipeline(text, forced_bos_token_id = pipe.tokenizer.get_lang_id(self.lang))[0][\"generated_text\"]\n",
    "    \n",
    "    def __call__(self, question):\n",
    "        \n",
    "        # Translate question to English\n",
    "        en_question = self.translate_lang_to_en(question)\n",
    "        \n",
    "        # Obtain the response in English\n",
    "        en_response = super().__call__(en_question)\n",
    "        \n",
    "        # Translate the response to the bots language\n",
    "        lang_response = self.translate_en_to_lang(en_response)\n",
    "        \n",
    "        return lang_response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef9c9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "matt_hi = MultilingualMathAssistant(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73b09a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   32        42.   35   ,    32 + 42 - 35 = 39   .  39 .'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt_hi(\"   32        42   35  ,       ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e55dac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   $ 23 .   $ 3   5  ,   $ 15  .  $ 23  - $ 15 = $ 8.  $ 8 .'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matt_hi(\"   23    $ 3            ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a26572",
   "metadata": {},
   "source": [
    "Now, that we have all the components ready for building our mutlilingual Math Assistant, we can create a we app using [Gradio](https://gradio.app/), to build an interactive solution which we can later deploy! Head on to [app.py](app.py) for the implementation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41684145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
