{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Auto-Regressive Decoding in Language Models (Seq2Seq and Decoder only models)\n\nOne final unit of decoding LLMs (pun intended :) involves understanding how to actually generate sequences given a representation of the input. We'll do a light exercise on the most basic run of the decoding here and then you can combine this pipeline with the decoding methods discussed in the previous class to observe variations in different kind of outputs. ","metadata":{"id":"x-Rpw1oIi31g"}},{"cell_type":"code","source":"!pip install transformers \n!pip install sentencepiece\n!pip install torch\n!pip install sacremoses","metadata":{"id":"opbkyW9wjS3A","execution":{"iopub.status.busy":"2023-03-25T08:16:43.046919Z","iopub.execute_input":"2023-03-25T08:16:43.047378Z","iopub.status.idle":"2023-03-25T08:17:36.663392Z","shell.execute_reply.started":"2023-03-25T08:16:43.047340Z","shell.execute_reply":"2023-03-25T08:17:36.662014Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.0+cpu)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacremoses) (2021.11.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses) (8.1.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sacremoses) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses) (4.11.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (3.11.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (4.4.0)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1939135a268201c0ac2fb711a471f26579916940611c2fee15e303e2480aad94\n  Stored in directory: /root/.cache/pip/wheels/5b/e0/77/05245143a5b31f65af6a21f7afd3219e9fa4896f918af45677\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.0.53\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\nimport torch\nimport numpy as np\n\ntokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\nmodel = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n\n# create ids of encoded input vectors\ninput_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\nprint(f'{input_ids} are the input ids')\n\n\ndecoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\nprint(f'{decoder_input_ids} is the decoder input ids')\n\n# let's feed this input to our model\noutputs = model(input_ids, decoder_input_ids = decoder_input_ids, return_dict=True)\n\nencoded_sequence = (outputs.encoder_last_hidden_state,)\nprint(encoded_sequence)\n# now that we have our inputs representation, let's decode \n\nwhile True: \n  # pass our encoder representation and decoder start id to our decoder \n  lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n\n  # pick the likeliest token \n  next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n  print(next_decoder_input_ids)\n  \n  # concatenate that with our current decoder ids \n  decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n  print(decoder_input_ids)\n  print(f\"Generated so far: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}\")\n\n  # stop when you encounted the <eos>\n  if next_decoder_input_ids == tokenizer.eos_token_id:\n    print(f'Final translation is {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}')\n    break \n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYooVg9hijer","outputId":"dab2b7f0-403a-475d-f8f9-5421748e21b5","execution":{"iopub.status.busy":"2023-03-25T08:17:36.666269Z","iopub.execute_input":"2023-03-25T08:17:36.666655Z","iopub.status.idle":"2023-03-25T08:18:03.747121Z","shell.execute_reply.started":"2023-03-25T08:17:36.666619Z","shell.execute_reply":"2023-03-25T08:18:03.745537Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"571f31397e1548fa8c2923cf821bb028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff7cecd0ae44e6fa7131d4679dd4c5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f734d3e09a524647ba5b79eacb34ab7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b594b490a3a5419d81e58c55e9568e0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84ad5026295403da63ba0f2aad74f1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d1d1dca11149a1a0e5667903853ed7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8f6322125c841ee904ab1fea321bd18"}},"metadata":{}},{"name":"stdout","text":"tensor([[  56,  385,    7, 5333,   19, 3869,    0]]) are the input ids\ntensor([[61949]]) is the decoder input ids\n(tensor([[[ 0.9560, -0.1123,  0.1342,  ..., -0.2770,  0.3980,  1.0380],\n         [-0.4514,  0.4379, -0.3578,  ..., -0.3392, -0.6470,  0.2711],\n         [-0.1239,  0.9591, -0.3312,  ...,  0.0969, -0.0748,  0.2844],\n         ...,\n         [-0.0996,  0.5244,  0.5465,  ...,  0.6494, -0.2613, -0.1096],\n         [-0.1579,  0.0308,  0.6918,  ..., -0.1743,  0.2450, -0.2864],\n         [-0.0543, -0.1454, -0.0861,  ..., -0.0810, -0.1390,  0.1813]]],\n       grad_fn=<NativeLayerNormBackward0>),)\ntensor([[104]])\ntensor([[61949,   104]])\nGenerated so far: मैं\ntensor([[38]])\ntensor([[61949,   104,    38]])\nGenerated so far: मैं एक\ntensor([[3444]])\ntensor([[61949,   104,    38,  3444]])\nGenerated so far: मैं एक कार\ntensor([[10261]])\ntensor([[61949,   104,    38,  3444, 10261]])\nGenerated so far: मैं एक कार खरीद\ntensor([[448]])\ntensor([[61949,   104,    38,  3444, 10261,   448]])\nGenerated so far: मैं एक कार खरीदना\ntensor([[486]])\ntensor([[61949,   104,    38,  3444, 10261,   448,   486]])\nGenerated so far: मैं एक कार खरीदना चाहता\ntensor([[254]])\ntensor([[61949,   104,    38,  3444, 10261,   448,   486,   254]])\nGenerated so far: मैं एक कार खरीदना चाहता हूँ\ntensor([[0]])\ntensor([[61949,   104,    38,  3444, 10261,   448,   486,   254,     0]])\nGenerated so far: मैं एक कार खरीदना चाहता हूँ\nFinal translation is मैं एक कार खरीदना चाहता हूँ\n","output_type":"stream"}]},{"cell_type":"code","source":"print(np.shape(encoded_sequence[0]), np.shape(lm_logits[0]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TisrC_oXwrgZ","outputId":"2fec5f98-de4e-4bf3-d62b-230229ba3539","execution":{"iopub.status.busy":"2023-03-25T08:18:03.749531Z","iopub.execute_input":"2023-03-25T08:18:03.750849Z","iopub.status.idle":"2023-03-25T08:18:03.762565Z","shell.execute_reply.started":"2023-03-25T08:18:03.750791Z","shell.execute_reply":"2023-03-25T08:18:03.758858Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"torch.Size([1, 7, 512]) torch.Size([8, 61950])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"But what about a model that does not have an encoder ?  Current models like GPT (Generative, Pretrained Models) do not have an encoder. How does one decode in those scenarios ? ","metadata":{"id":"FuwE_RO3mtOJ"}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\ntext = \"I went \"\nencoded_input = tokenizer.encode(text, return_tensors='pt')\noutput = model(input_ids = encoded_input)\n\nwhile True:\n  logits = output.logits\n  next_decoder_input_ids = torch.argmax(logits[:, -1:], axis=-1)\n  encoded_input = torch.cat([encoded_input, next_decoder_input_ids], axis=-1)\n  print(f\"Current Generation: {tokenizer.decode(encoded_input[0], skip_special_tokens=True)}\")\n  if len(encoded_input[0]) >= 30: \n    print(f\"Complete Generation: {tokenizer.decode(encoded_input[0], skip_special_tokens=True)}\")\n    break \n  \n  output = model(encoded_input)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISp5onfIpyn9","outputId":"bf795a51-509d-4abb-85b4-9d8262f0e137","execution":{"iopub.status.busy":"2023-03-25T08:18:03.765874Z","iopub.execute_input":"2023-03-25T08:18:03.767110Z","iopub.status.idle":"2023-03-25T08:18:17.721949Z","shell.execute_reply.started":"2023-03-25T08:18:03.767055Z","shell.execute_reply":"2023-03-25T08:18:17.720560Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319f354cc1fe4eb299efa4e33edd7c63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd256cf455e944a293b47b8a69c86909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dddf3262dcb345b39c6c8690f7def087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a6165f67cd49719fa85d72f1e674c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015ddade768143e1a31dd05a9174c451"}},"metadata":{}},{"name":"stdout","text":"Current Generation: I went  \nCurrent Generation: I went  to\nCurrent Generation: I went  to the\nCurrent Generation: I went  to the \nCurrent Generation: I went  to the University\nCurrent Generation: I went  to the University of\nCurrent Generation: I went  to the University of California\nCurrent Generation: I went  to the University of California,\nCurrent Generation: I went  to the University of California, Berkeley\nCurrent Generation: I went  to the University of California, Berkeley,\nCurrent Generation: I went  to the University of California, Berkeley, and\nCurrent Generation: I went  to the University of California, Berkeley, and I\nCurrent Generation: I went  to the University of California, Berkeley, and I was\nCurrent Generation: I went  to the University of California, Berkeley, and I was there\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester.\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a year\nCurrent Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a year and\nComplete Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a year and\n","output_type":"stream"}]},{"cell_type":"markdown","source":"That's it! Now mix and match this decoding pipeline with the methods that we have discussed before to understand the effect of adopting different sampling strategies on top of this autoregressive pipelines. Now let's head over to https://chat.openai.com/ to see a few quirks of this generation. ","metadata":{"id":"QZyUCY6TyybB"}},{"cell_type":"code","source":"","metadata":{"id":"51Mayf0Oqr1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}