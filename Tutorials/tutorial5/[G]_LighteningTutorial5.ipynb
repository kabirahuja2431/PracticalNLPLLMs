{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Auto-Regressive Decoding in Language Models (Seq2Seq and Decoder only models)\n",
        "\n",
        "One final unit of decoding LLMs (pun intended :) involves understanding how to actually generate sequences given a representation of the input. We'll do a light exercise on the most basic run of the decoding here and then you can combine this pipeline with the decoding methods discussed in the previous class to observe variations in different kind of outputs. "
      ],
      "metadata": {
        "id": "x-Rpw1oIi31g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers \n",
        "!pip install sentencepiece\n",
        "!pip install torch\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "opbkyW9wjS3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "\n",
        "# create ids of encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
        "print(f'{input_ids} are the input ids')\n",
        "\n",
        "\n",
        "decoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "print(f'{decoder_input_ids} is the decoder input ids')\n",
        "\n",
        "# let's feed this input to our model\n",
        "outputs = model(input_ids, decoder_input_ids = decoder_input_ids, return_dict=True)\n",
        "\n",
        "encoded_sequence = (outputs.encoder_last_hidden_state,)\n",
        "print(encoded_sequence)\n",
        "# now that we have our inputs representation, let's decode \n",
        "\n",
        "while True: \n",
        "  # pass our encoder representation and decoder start id to our decoder \n",
        "  lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
        "\n",
        "  # pick the likeliest token \n",
        "  next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
        "  print(next_decoder_input_ids)\n",
        "  \n",
        "  # concatenate that with our current decoder ids \n",
        "  decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
        "  print(decoder_input_ids)\n",
        "  print(f\"Generated so far: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}\")\n",
        "\n",
        "  # stop when you encounted the <eos>\n",
        "  if next_decoder_input_ids == tokenizer.eos_token_id:\n",
        "    print(f'Final translation is {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}')\n",
        "    break \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYooVg9hijer",
        "outputId": "dab2b7f0-403a-475d-f8f9-5421748e21b5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  56,  385,    7, 5333,   19, 3869,    0]]) are the input ids\n",
            "tensor([[61949]]) is the decoder input ids\n",
            "(tensor([[[ 0.9560, -0.1123,  0.1342,  ..., -0.2770,  0.3980,  1.0380],\n",
            "         [-0.4514,  0.4379, -0.3578,  ..., -0.3392, -0.6470,  0.2711],\n",
            "         [-0.1239,  0.9591, -0.3312,  ...,  0.0969, -0.0748,  0.2844],\n",
            "         ...,\n",
            "         [-0.0996,  0.5244,  0.5465,  ...,  0.6494, -0.2613, -0.1096],\n",
            "         [-0.1579,  0.0308,  0.6918,  ..., -0.1743,  0.2450, -0.2864],\n",
            "         [-0.0543, -0.1454, -0.0861,  ..., -0.0810, -0.1390,  0.1813]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>),)\n",
            "tensor([[104]])\n",
            "tensor([[61949,   104]])\n",
            "Generated so far: मैं\n",
            "tensor([[38]])\n",
            "tensor([[61949,   104,    38]])\n",
            "Generated so far: मैं एक\n",
            "tensor([[3444]])\n",
            "tensor([[61949,   104,    38,  3444]])\n",
            "Generated so far: मैं एक कार\n",
            "tensor([[10261]])\n",
            "tensor([[61949,   104,    38,  3444, 10261]])\n",
            "Generated so far: मैं एक कार खरीद\n",
            "tensor([[448]])\n",
            "tensor([[61949,   104,    38,  3444, 10261,   448]])\n",
            "Generated so far: मैं एक कार खरीदना\n",
            "tensor([[486]])\n",
            "tensor([[61949,   104,    38,  3444, 10261,   448,   486]])\n",
            "Generated so far: मैं एक कार खरीदना चाहता\n",
            "tensor([[254]])\n",
            "tensor([[61949,   104,    38,  3444, 10261,   448,   486,   254]])\n",
            "Generated so far: मैं एक कार खरीदना चाहता हूँ\n",
            "tensor([[0]])\n",
            "tensor([[61949,   104,    38,  3444, 10261,   448,   486,   254,     0]])\n",
            "Generated so far: मैं एक कार खरीदना चाहता हूँ\n",
            "Final translation is मैं एक कार खरीदना चाहता हूँ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(encoded_sequence[0]), np.shape(lm_logits[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TisrC_oXwrgZ",
        "outputId": "2fec5f98-de4e-4bf3-d62b-230229ba3539"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 512]) torch.Size([8, 61950])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what about a model that does not have an encoder ?  Current models like GPT (Generative, Pretrained Models) do not have an encoder. How does one decode in those scenarios ? "
      ],
      "metadata": {
        "id": "FuwE_RO3mtOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "text = \"I went \"\n",
        "encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
        "output = model(input_ids = encoded_input)\n",
        "\n",
        "while True:\n",
        "  logits = output.logits\n",
        "  next_decoder_input_ids = torch.argmax(logits[:, -1:], axis=-1)\n",
        "  encoded_input = torch.cat([encoded_input, next_decoder_input_ids], axis=-1)\n",
        "  print(f\"Current Generation: {tokenizer.decode(encoded_input[0], skip_special_tokens=True)}\")\n",
        "  if len(encoded_input[0]) >= 30: \n",
        "    print(f\"Complete Generation: {tokenizer.decode(encoded_input[0], skip_special_tokens=True)}\")\n",
        "    break \n",
        "  \n",
        "  output = model(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISp5onfIpyn9",
        "outputId": "bf795a51-509d-4abb-85b4-9d8262f0e137"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Generation: I went  \n",
            "Current Generation: I went  to\n",
            "Current Generation: I went  to the\n",
            "Current Generation: I went  to the \n",
            "Current Generation: I went  to the University\n",
            "Current Generation: I went  to the University of\n",
            "Current Generation: I went  to the University of California\n",
            "Current Generation: I went  to the University of California,\n",
            "Current Generation: I went  to the University of California, Berkeley\n",
            "Current Generation: I went  to the University of California, Berkeley,\n",
            "Current Generation: I went  to the University of California, Berkeley, and\n",
            "Current Generation: I went  to the University of California, Berkeley, and I\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester.\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a year\n",
            "Current Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a year and\n",
            "Complete Generation: I went  to the University of California, Berkeley, and I was there for a semester. I was a student there for a year and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! Now mix and match this decoding pipeline with the methods that we have discussed before to understand the effect of adopting different sampling strategies on top of this autoregressive pipelines. Now let's head over to https://chat.openai.com/ to see a few quirks of this generation. "
      ],
      "metadata": {
        "id": "QZyUCY6TyybB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "51Mayf0Oqr1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}